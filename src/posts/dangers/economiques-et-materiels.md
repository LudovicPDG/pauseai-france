---
title: Dangers économiques et matériels
description: Les dangers économiques et matériels de l'IA incluent des impacts sur les infrastructures et les entreprises, de nouvelles vulnérabilités, et des défis d'automatisation, nécessitant une gestion prudente via la régulation et l'innovation technologique.
---

Les dangers économiques et matériels liés à l'intelligence artificielle concernent principalement les impacts directs sur les infrastructures, les entreprises, et les ressources matérielles. Alors que l'IA promet des avancées significatives dans l'optimisation et l'automatisation des processus, elle introduit également des vulnérabilités nouvelles et des possibilités de dysfonctionnements. Ces dangers comprennent la facilitation d’attaques à l’aide d’IA, les dysfonctionnements prosaïques et l’impact économique de technologies de rupture. Bien qu’il soit important de gérer les dangers qui menacent des personnes humaines, ceux-ci ne sont pas non plus à négliger.

La gestion de ces dangers nécessite une vigilance accrue, des régulations rigoureuses et surtout des innovations technologiques pour garantir la résilience et la sécurité des systèmes d’IA et de systèmes confrontés à l’IA.

## Cyberattaques

Lorsque l’IA surpassera les meilleurs programmeurs humains, elle pourra être utilisée pour créer de nouveaux types de cyberattaques et découvrir de nouvelles vulnérabilités. Elle peut déjà être utilisée pour automatiser les attaques afin d’augmenter leur étendue et les rendre adaptives aux mesures de sécurité.

L’IA peut aussi être utilisée pour renforcer la cyberdéfense des systèmes, mais de nombreux systèmes n’en prendront pas la peine. De plus, la cybersécurité est un domaine asymétrique où il est plus facile de découvrir/introduire des vulnérabilités que de les corriger.

Les cyberattaques peuvent aussi causer des morts humaines ou de la désorganisation à grande échelle (par exemple en paralysant un hôpital ou un bureau gouvernemental).

Ce danger peut être évité en régulant le déploiement d’IA à fortes capacités de programmation.

## Perte de fonction par défaut

A cause de problèmes de mauvaise généralisation des objectifs ou de détournement des spécifications, il est difficile de garantir que des systèmes d’IA auront un comportement correct après leur déploiement. Si ces systèmes sont une partie critique d’un processus économique, leur dysfonctionnement peut causer des dommages considérables.

Certains de ces dysfonctionnements sont issus d’un manque de données et de capacités et seront probablement réglés alors que la technologie mature.

Si vous utilisez des systèmes d’IA à vos propres fins, tenez-vous au courant des limitations de vos outils, de leurs modes d’échecs connus et contrôlez la qualité de leurs sorties.

Ce danger peut être mitigé en contrôlant la qualité des systèmes d’IA et en poussant plus loin les techniques actuelles d’alignement. De nouvelles techniques d’alignement sont nécessaires pour l’éviter prouvablement.

## Perte de fonction par intervention extérieure

Un système d’IA est plus à risque de dysfonctionnement lorsqu’il se trouve en-dehors de ses conditions normales d’utilisation - on parle de sortie de distribution lorsque sa situation est différente de celles sur lesquelles il a été entraîné. On tente autant que possible d’améliorer la robustesse des IA hors distribution de nos jours, mais des techniques d’attaques adverses, qui consistent pour un acteur opposé à générer une situation de façon à éliciter un comportement erroné, posent problème.

Pour des systèmes généralistes, il est impossible d’explorer tous les cas d’utilisation possibles afin de les corriger, et il est relativement facile de contourner les précautions de sécurité ad hoc qui empêchent l’utilisateur moyen d’utiliser ces systèmes à des fins néfastes.

De plus, l’utilisation de quantités massives de données impossibles à contrôler par un humain ouvre la voie à l’empoisonnement de données qui introduisent des vulnérabilités dans les systèmes qui s’entraînent dessus.

En sens contraire, les attaques adverses peuvent être utilisées pour renforcer la sécurité d’un système et l’empoisonnement de données permet à des acteurs indépendants de lutter contre le vol de données.

Ces dangers sont l’objet de beaucoup de recherches motivées par des intérêts économiques et de souveraineté donc la dynamique entre attaque et défense fluctue rapidement. Ils peuvent être influencés par des actions à toutes les étapes : régulation, standards, alignement technique, sécurité au déploiement, etc.

## Takeover entrepreneurial

Les nombreuses applications de l’IA présentent des opportunités d’innovations de rupture qui permettraient à une entreprise de gagner rapidement en valeur et de s’imposer à travers la société. Alors que cette possibilité est généralement acceptée ou désirée comme une conséquence normale de notre système économique, les applications de l’IA à la surveillance, à la persuasion, à la médecine et à l’armement devraient nous rendre circonspect pour une fois : voulons-nous d’une entreprise globale qui domine notre capacité à nous soigner, ou dont l’aval est nécessaire pour avoir des opinions individuelles protégées des outils de manipulation en ligne ?

Un takeover entrepreneurial, quoiqu’on puisse débattre de sa désirabilité en soi, diminuerait la responsabilité des fournisseurs d’IA et paverait la voie à des abus d’une ampleur sans précédent.

Ce danger peut être mitigé par des régulations, des actions en justice et un engagement citoyen sur le sujet.
