---
title: Dangers pour les individus
description: L'intégration de l'IA dans la vie quotidienne présente des dangers pour la vie privée, la sécurité et le bien-être des individus, nécessitant des régulations, une éducation du public et des systèmes d'IA sécurisés pour les atténuer.
---

L’intégration de l’IA dans la vie quotidienne pose des dangers significatifs pour les individus, touchant à leur vie privée, leur sécurité et leur bien-être. Ces dangers incluent la violation de la confidentialité, la propagation de fausses informations et la manipulation psychologique, avec pour conséquence des dommages émotionnels ou physiques graves. Afin d’atténuer ces dangers, il est crucial de mettre en place des cadres réglementaires robustes et des pratiques de sensibilisation et d’éducation auprès du public, ainsi que des gardes-fous intégrés aux systèmes d’IA.

## Violation de la vie privée et de la confidentialité

L’intégration de systèmes d’IA dans des objets pervasifs de notre société tels que les ordinateurs ou les caméras de surveillance, à des fins d’analyse locale ou de collecte de données [centralise les données privées](https://forum.effectivealtruism.org/posts/zd5inbT4kYKivincm/ai-is-centralizing-by-default-let-s-not-make-it-worse) ce qui rend les fuites plus graves. De plus, les données d’utilisation des systèmes d’IA sont généralement [collectées par les développeurs](https://openai.com/policies/privacy-policy/) pour améliorer leurs produits et se retrouvent indirectement dans les IA futures.

Pour ne pas être victime, vous pouvez ne pas partager vos données personnelles, vos travaux ou des données confidentielles avec des IA, mais cela restreint fortement l’usage que vous pouvez en faire.

Ce danger peut être mitigé en régulant l’usage et la protection des données par les fournisseurs d’IA. Des [techniques d’empoisonnement](https://www.computer.org/csdl/proceedings-article/sp/2024/313000a212/1WPcYDRkVX2) pourraient aider à lutter contre les abus.

## Désinformation et Deepfakes

Les IA génératives, notamment de voix et d’images, peuvent être utilisées pour reproduire l’apparence et la voix de quelqu’un de précis [à partir de quelques données récoltées en ligne](https://www.thetimes.com/business-money/technology/article/h-how-to-make-deepfake-video-easy-six-minutes-d57s9hswr).

Au-delà des inquiétudes éthiques, les deepfakes peuvent contribuer à des arnaques et à la propagation de fausses informations, [personnalisée](https://www.newyorker.com/science/annals-of-artificial-intelligence/the-terrifying-ai-scam-that-uses-your-loved-ones-voice) ou [généralisée](https://www.lemonde.fr/pixels/article/2024/02/26/deepfake-de-joe-biden-l-identite-du-commanditaire-devoilee_6218633_4408996.html). Les outils de création de deepfakes réduisent la barrière technique et psychologique à la falsification de contenu en ligne. N’est pas vrai tout ce que qu’on lit sur Internet, et désormais il est difficile de faire confiance même à des images ou des enregistrements audio.

Pour ne pas être victime d’un deepfake, de façon directe ou pervasive, tenez-vous au courant de ce qui peut être falsifié et assainissez votre consommation de contenu en ligne. Cela est malheureusement difficile à appliquer à l’échelle de la société.

Ce danger peut être mitigé par des régulations gouvernementales. Une réforme radicale de nos systèmes d’information sera nécessaire pour éviter complètement ce danger, mais le sujet est fortement débattu.

## Algorithmes de recommandation et manipulation invisible

Les algorithmes de recommandation, présents sur les réseaux sociaux, les moteurs de recherche et les plateformes de streaming, orientent discrètement nos choix, en proposant des contenus optimisés pour capter notre attention. Derrière cette apparente commodité se cachent des risques significatifs à l'échelle de l'individu mais également à l'échelle de la société : pour [nos démocraties](https://www.com.cuhk.edu.hk/publication/michael-journal-2024-social.pdf), [notre santé mentale](https://pmc.ncbi.nlm.nih.gov/articles/PMC10476631/) et notre capacité à distinguer le vrai du faux.

En favorisant les contenus émotionnellement chargés et polarisants, ces systèmes exacerbent [les divisions politiques](https://www.polytechnique-insights.com/en/columns/digital/are-recommendation-algorithms-a-source-of-polarization/) et amplifient [les discours haineux](https://policyreview.info/articles/analysis/recommender-systems-and-amplification-extremist-content). Leur capacité à propager rapidement de la désinformation les rend particulièrement dangereux en période critique, comme lors [des élections](https://commons.clarku.edu/sps_masters_papers/91/). De plus, l’exposition prolongée à des contenus optimisés pour maintenir l’engagement peut nuire au bien-être psychologique, en particulier chez les jeunes.

Ce danger peut être atténué par une transparence accrue dans le fonctionnement de ces systèmes et des régulations adaptées pour encadrer leur usage. Une prise de conscience collective est également essentielle pour apprendre à identifier et à limiter leur influence.

## Tromperie stratégique

Un système d’IA pourrait recourir à la tromperie pour atteindre ses fins, qu’il ait appris ce comportement par imitation ou généralisation de mensonges humains, ou qu’il s’agisse d’une capacité développée par le système lui-même.

Un tel système pourrait résister activement aux tentatives de supervision, de contrôle et de correction de ses développeurs et causer des dommages économiques, émotionnels ou physiques aux humains qu’il manipule. Si ses objectifs sont néfastes à la société, il pourrait aussi causer des dégâts de par ses efforts pour atteindre ses objectifs.

Les [études actuelles](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11117051/) montrent déjà des capacités émergentes de tromperie, qui pourraient poser problème dans les IA futures.

Ce danger peut être mitigé par [un contrôle à l’entraînement et au déploiement des IA](https://pauseia.fr/propositions) de pointe. Il ne pourra être évité qu’en inventant de meilleures techniques d’alignement.
