---
title: Dangers pour l’humanité
description: Les dangers existentiels de l'IA, bien que spéculatifs, menacent l'avenir de l'humanité et nécessitent une réflexion anticipée et des recherches en alignement technique, car la régulation seule ne peut contrer l'attrait économique du progrès dans ce domaine.
---

Plus spéculativement, l’IA pose des dangers qui menacent durablement le futur de l’humanité. Ces dangers ne peuvent pas forcément être observés, et il est peu probable que nous aurons des sirènes d’alarme plus claires qu’aujourd’hui avant qu’il ne soit trop tard. C’est pourquoi il est nécessaire d’y réfléchir même s'il existe peu d’arguments empiriques pour aider à y voir plus clair. Ces dangers apocalyptiques sont liés au potentiel extraordinaire de l’IA générale, et passent par des innovations technologiques qui pourraient arriver dans 6 mois ou dans 10 ans.

La recherche en alignement technique est notre meilleur espoir d’éviter ces dangers car des mesures régulatoires ne peuvent que les ralentir. On n’arrête pas le progrès quand il y a des milliards de dollars à la clé.

## Réplication autonome adaptative (ARA)

La concordance de quelques capacités que divers systèmes d’IA vont probablement acquérir dans les prochaines années permettrait des agents informatiques difficiles à arrêter tandis qu’ils poursuivraient des objectifs qui leur sont propres, par exemple des virus informatiques qui évoluent continuellement de manière à prendre le contrôle d’un maximum de machines.

Ces capacités sont :

- l’auto-réplication, la capacité de dupliquer son code source (c’est-à-dire ses paramètres associées à un échafaudage approprié) et le propager d’un support à un autre.
- l’autonomie, la capacité d’opérer en continu indépendamment d’un utilisateur humain.
- l’adaptation, la capacité de se modifier afin de mieux répondre à sa situation, ses besoins et les mesures adverses.

Les progrès de l’IA en programmation, en compréhension générale et les techniques d’échafaudage nous rapprochent très vite d’ARA et notre société actuelle n’est pas prête à intervenir efficacement contre une IA dotée d’ARA ; il est donc nécessaire d’étudier ce danger au plus vite, et de prendre des mesures pour empêcher qu’il se concrétise dès maintenant.

Ce danger peut être contré à plusieurs étapes : il est de la responsabilité des États de mettre en place des régulations qui empêchent la création ou la mise en service d’IA capables d’ARA ; il revient aux entreprises de mettre en place des processus de supervision et d’évaluation lors de l’entraînement et avant le déploiement ; il est nécessaire de pousser la recherche sur le sujet pour trouver un moyen d’éviter durablement ce danger.

## Superintelligence désalignée

L’IA a un certain nombre d’avantages intrinsèques sur l’intelligence humaine : de meilleures capacités de duplication et de coordination, la vitesse de reproduction, un plafond de capacités générales supérieur, un substrat mieux adapté à la cognition, etc.

Pour diverses raisons, il n’est pas impossible qu’une IA qui dépasse largement les capacités cognitives humaines dans de nombreux domaines ait des valeurs défavorables aux intérêts de l’humanité et nous élimine dans la poursuite de ses objectifs.

Le sujet est largement débattu, certains experts en IA considérant que la possibilité est trop incertaine tandis que d’autres pensent qu’il s’agit de l’enjeu principal de la recherche en IA, mais ce danger est généralement reconnu comme la plus grande source de danger d’extinction humaine due à l’IA.

Ce danger requiert de la recherche en alignement technique, et une solution peut être facilitée par des régulations comme une pause du développement d’IA de frontières. Il est impossible à mitiger.

## Perte de contrôle

En s’appuyant de plus en plus sur des systèmes d’IA pour des infrastructures critiques au fonctionnement de la société telles que le transport, l’information et la surveillance, nous optimisons de plus en plus efficacement pour des mesures qui correspondent plus ou moins aux valeurs humaines (par exemple l’incarcération des criminels plutôt que la réduction des crimes).

En faisant passer les humains d’opérateurs à superviseurs, en nous éloignant de plus en plus des actuateurs, nous courons le danger de ne plus pouvoir contrôler notre société, étant à la merci de dynamiques trop abstraites, informatiques, compliquées et/ou obfusquées pour pouvoir les modifier, soit qu’un effort concerté n’ait pas assez d’impact pour modifier même des systèmes critiques (comme il est déjà difficile de le faire sans violence pour un gouvernement corrompu) ou soit qu’il soit impossible de coordonner la société à faire ce genre d’effort.

Dans cette situation, l’humanité pourrait durablement survivre dans une dystopie, et ne jamais réaliser son potentiel de prospérité, ce qui serait une catastrophe morale sans mesure.
Ce danger provient de l’accumulation des précédents dangers systémiques et peut être contré à plusieurs niveaux selon les mesures techniques, industrielles et gouvernementales précédemment évoquées.
