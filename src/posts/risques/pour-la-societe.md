---
title: Risques pour la société
description: Les risques sociétaux de l'IA, incluant des menaces pour la démocratie, la cohésion sociale et l'emploi, nécessitent une régulation proactive, une refonte des systèmes d'information et une éducation à la pensée critique pour préserver nos valeurs et structures sociales.
---

Certains risques touchent la société entière, et doivent être considérés dans cette perspective afin de ne pas en négliger des aspects majeurs. Les risques pour la société liés à l'IA sont vastes et touchent directement à la cohésion sociale et aux fondements de la démocratie. Ces risques portent sur l'intégrité des processus démocratiques par l’affaiblissement, la polarisation et la désinformation, mais aussi sur les processus sociaux comme les biais raciaux ou l’automatisation du travail, ainsi que sur des problèmes plus directs comme la guerre et le terrorisme.

Ces risques émergeant souvent d’un usage mal avisé des nouvelles technologies, une régulation proactive, une refonte des systèmes d'information et une éducation à la pensée critique sont les meilleures mesures afin de préserver nos valeurs et nos structures sociales.

## Perpétuation des biais

Les IA apprennent de leurs données d’entraînement, et on leur demande souvent d’imiter le comportement d’humains, y compris leurs biais raciaux ou de genre.

Alors que des entreprises d’IA fleurissent à travers la France et que son usage se démocratise à toutes les échelles de la société, ces biais risquent de se répandre et de devenir impossibles à éliminer.

Ce risque doit être mitigé par les fournisseurs d’IA qui sont responsables des erreurs de leurs produits en dépensant plus d’efforts à cerner et corriger les biais. Il est aussi recommandé aux individus et entreprises de se tenir informés pour ne pas être complices dans la perpétuation des biais.

## Détérioration de l’épistémologie

Par le passé, il était difficile d’accéder à une information lorsqu’elle existait dans un endroit idéologiquement ou géographiquement lointain. De nos jours, il est difficile de vérifier une information, car il existe trop de bruit, d’erreurs et de mensonges, ce qui réduit notre exposition à des opinions contradictoires. Les systèmes de recommandation conçus pour maximiser l’engagement des utilisateurs aggravent ce problème en nous poussant vers des contenus dont nous approuvons plutôt que vers des contenus informants, ce qui favorise la création de chambres d’échos. De plus, la facilitation de la génération de contenu faux rend d’autant plus facile le fait d’accuser ses opposants idéologiques de falsifier leurs affirmations par l’IA, ce qui nuit à la santé du débat public.

Tous ces problèmes accumulés nous conduisent vers un futur où chercher la vérité devient difficile personnellement et socialement.

Un individu peut se garder de ce risque par des efforts d’hygiène mentale, mais le risque porte sur une perte générale de la capacité à s’informer par soi-même, ce qui pose des problèmes à l’échelle de la société même lorsque certains individus sont immunisés.

Ce risque peut être combattu par des campagnes d’information et d’éducation, par une régulation des réseaux sociaux et autres systèmes d’information, mais il nécessite surtout une refonte complète de notre rapport à l’information.

## Polarisation et affaiblissement

L’IA peut être utilisée à des fins politiques et idéologiques, en exploitant ses capacités à manipuler ou persuader des humains et à sélectionner du contenu qui influencera leurs émotions.

Des acteurs étatiques peuvent notamment cibler la population de pays opposés pour la pousser à l’instabilité et au mécontentement, en générant et en boostant la visibilité de contenus controversés, comme [la Russie l’a fait pour la France](https://downloads.ctfassets.net/kftzwdyauwt9/5IMxzTmUclSOAcWUXbkVrK/3cfab518e6b10789ab8843bcca18b633/Threat_Intel_Report.pdf). Similairement, il n’est peut-être pas impossible de saper systématiquement les capacités mentales d’un pays en exposant sa population à des contenus démoralisants ou abrutissants, comme la Chine est soupçonnée de le faire à travers TikTok.

Ce risque doit être géré par des interventions du gouvernement pour en identifier les vecteurs et les contrer. Idéalement, des accords internationaux garantiraient que ce genre d’armes informationnelles ne soient pas utilisées.

## Guerre automatisée

Les drones et robots sont de plus en plus utilisés dans les armées à la pointe de la technologie. A première vue, il s’agit d’un développement positif, en réduisant l’investissement et le coût humain de la guerre.

Mais il n’en est pas de même quant à l’automatisation des prises de décision. Les dérapages des IA touchent alors à des systèmes critiques qui peuvent facilement attaquer des populations civiles. On risque littéralement un scénario à la Terminator.

Les armées qui refusent d’automatiser leur hiérarchie courent le risque d’être moins efficaces, notamment en termes de vitesse de réaction. Mais dans l’autre sens, deux systèmes stratégiques automatisés pourraient aggraver une situation tendue pour des raisons confuses à grande vitesse.

Il y a donc une dynamique perverse qui pousse les armées à faire courir des risques d’entrée en guerre au reste du monde, et de guerres plus totales, afin de ne pas être dépassées technologiquement.

Ce risque doit être contré par des accords internationaux pour éviter la mise en service de systèmes d’IA militaires et de régulations nationales pour éviter leur développement.

## Bioterrorisme

L’IA peut être utilisée pour démocratiser la conception de composants chimiques toxiques et dans le futur pourrait permettre la conception de pathogènes sans besoin d’expertise humaine. Les acteurs mal intentionnés, limités principalement par un manque de moyens et d’expertise, pourraient entreprendre des actions terroristes à une échelle nationale voire planétaire en empoisonnant des sources d’eau ou en déployant des pandémies artificielles.

Ce risque peut être mitigé en préparant nos systèmes de défense et sanitaire pour tenir compte de cette possibilité. Il peut être évité en contrôlant le développement et l’utilisation d’IA capables de faciliter des actions terroristes.

## Enracinement de l’oppression

De même que les IA persuasives pourraient être utilisées pour déstabiliser un État, elles pourraient être utilisées au contraire pour asseoir son autorité en sapant l’opposition, notamment dans le cas d’un État autoritaire dénué de scrupules éthiques. Les technologies de surveillance, notamment aidées par la reconnaissance faciale et le profilage automatisé, pourraient efficacement contrôler la population tandis que les technologies d’information influenceraient les idées auxquelles elle serait exposée, garantissant ainsi la stabilité du système sans besoin de coercition physique.

Ce risque peut être évité en éduquant la population aux valeurs démocratiques. A vrai dire, il est moins susceptible de se concrétiser en France que dans des pays où l’IA est déjà largement utilisée à des fins de surveillance et de contrôle de l’information.
