---
title: F.A.Q.
description: Questions fréquemment posées sur Pause IA et les risques de l'IA superintelligente.
---

<!-- ↓↓↓ NE PAS TOUCHER ↓↓↓ -->
<script lang="ts">
  import Accordion from '$lib/components/Accordion.svelte'
	import { page } from '$app/stores'

  $: toc = $page.url.pathname === '/faq'

</script>

{#if toc}

<!-- ↑↑↑ NE PAS TOUCHER ↑↑↑ -->

## Sommaire

- [Qui êtes-vous ?](#accordion1)
- [N’êtes-vous pas tout simplement des technophobes ?](#accordion2)
- [Voulez-vous interdire toute forme d'IA ?](#accordion3)
- [Croyez-vous que GPT-4 va tous nous tuer ?](#accordion4)
- [Un moratoire ne risque-t-il pas d'aggraver les choses ?](#accordion5)
- [Un moratoire est-il possible ?](#accordion6)
- [Qui vous finance ?](#accordion7)
- [Quels sont vos projets ?](#accordion8)
- [Comment comptez-vous convaincre les gouvernements d'arrêter temporairement le développement de l'IA ?](#accordion9)
- [Pourquoi manifester ?](#accordion10)
- [Quelle est la probabilité que l'apparition d'une superintelligence ait de graves conséquences, y compris un risque d'extinction ?](#accordion11)
- [Combien de temps nous reste-t-il avant l'émergence d'une superintelligence ?](#accordion12)
- [Si nous appliquons un moratoire, qu'en est-il de la Chine ?](#accordion13)
- [OpenAI et Google semblent appeler de leurs voeux une réglementation. Pourquoi s'opposer à eux ?](#accordion14)
- [Les entreprises d'IA prétendent-elles que le risque existentiel est réel pour nous manipuler ?](#accordion15)
- [Je veux vous aider ! Que puis-je faire ?](#accordion16)

<!-- ↓↓↓ NE PAS TOUCHER ↓↓↓ -->

{/if}

<!-- ↑↑↑ NE PAS TOUCHER ↑↑↑ -->

### Qui êtes-vous ?

Nous sommes un ensemble de [bénévoles](https://pauseai.info/people) et de [groupes locaux](https://pauseai.info/communities), organisé par une [association à but non lucratif](/mentions-legales) dont l'objectif est de minimiser les [risques liés à l'IA](/risques) (y compris le [risque d'extinction](/risques/humanite)). Notre objectif est de convaincre nos gouvernements d'intervenir et [de mettre en pause le développement d'une IAG](/propositions) (Intelligence Artificielle Générale puis d’une Superintelligence). Dans ce but, nous alertons le public, dialoguons avec les décideurs et organisons des manifestations.

Vous pouvez nous rejoindre sur [Discord](https://discord.gg/vyXGd7AeGc) (C’est là que notre communauté est la plus active!), [Twitter](https://twitter.com/pause_ia), [Facebook](https://www.facebook.com/Pause.IA), [TikTok](https://www.tiktok.com/@pause_ia), [LinkedIn](https://www.linkedin.com/company/pause-ia/), [YouTube](https://www.youtube.com/@Pause_IA), [Instagram](https://www.instagram.com/pause_ia) et [Threads](https://www.threads.net/@pause_ia). Vous pouvez également nous contacter par mail à [contact@pauseia.fr](mailto:contact@pauseia.fr)

### N’êtes-vous pas tout simplement des technophobes ?

Vous seriez surpris d'apprendre que la plupart des membres de Pause IA sont favorablement disposés envers le progrès technologique. Nombre d'entre eux sont impliqués dans le développement de l'IA, sont des amateurs de nouvelles technologies et ont longtemps été très enthousiastes face à l'avenir. Beaucoup s’intéressaient particulièrement au potentiel de développement de l’humanité que recèle l’IA. C'est pourquoi, quand ils se sont rendu compte des risques existentiels liés à l’IA, nombre d’entre eux ont eu beaucoup de mal à [l’accepter / l’intégrer](https://pauseai.info/psychology-of-x-risk).

### Voulez-vous interdire toute forme d'IA ?

Non. Seulement le développement des plus gros systèmes d'IA à usage général souvent appelés "modèles de pointe". La quasi-totalité des modèles existants, ainsi que la plupart des futurs modèles d'IA, resteraient [légaux selon notre proposition](/propositions). Nous demandons l'interdiction des systèmes plus puissants que GPT-4-o, jusqu'à ce que nous puissions exercer un contrôle démocratique sur ces modèles et que nous soyons en mesure de les créer en toute sécurité.

### Croyez-vous que GPT-4 va tous nous tuer ?

Non, nous ne croyons pas que les [modèles actuels](https://pauseai.info/sota) représentent un risque existentiel. Probablement que la plupart des prochains modèles non plus. Mais si nous poursuivons le développement de systèmes toujours plus puissants, nous atteindrons un point de non-retour où l'un d'eux deviendra [une menace existentielle](/risques/humanite).

### Un moratoire ne risque-t-il pas d'aggraver les choses ?

Nous avons répondu à ces préoccupations [dans cet article](https://pauseai.info/mitigating-pause-failures).

### Un moratoire est-il possible ?

L’émergence d’une superintelligence n'est pas inévitable. Sa création nécessite des armées d'ingénieurs payés à coup de millions de dollars et une chaîne d'approvisionnement de matériel de pointe non réglementé. Sa création implique aussi que nous permettions à ces entreprises de jouer avec notre avenir en restant passifs.

[En savoir plus sur la faisabilité d’un moratoire .](https://pauseai.info/feasibility)

### Qui vous finance ?

Quasiment toutes nos actions jusqu'ici ont été menées par des bénévoles. Cependant, depuis février 2024, Pause IA est une [organisation à but non lucratif enregistrée](/mentions-legales), et nous avons reçu de multiples dons de particuliers. Nous avons également reçu 20 000 dollars de financement de la part du réseau LightSpeed.

Vous pouvez également [faire un don à Pause IA](/dons) si vous soutenez notre cause ! Nous utilisons l'essentiel de l'argent pour permettre à des communautés locales d'organiser des événements.

### Quels sont vos projets ?

Nous nous concentrons sur la [croissance du mouvement](https://pauseai.info/growth-strategy), l'organisation de manifestations, le lobbying auprès des politiciens et l'information du public.

Consultez [notre feuille de route](https://pauseai.info/roadmap) pour un aperçu détaillé de nos projets et de ce que nous pourrions faire avec plus de financements.

### Comment comptez-vous convaincre les gouvernements d’arrêter temporairement le développement de l'IA ?

Jetez un œil à notre "[Théorie du changement](https://pauseai.info/theory-of-change)" pour un aperçu détaillé de notre stratégie.

### Pourquoi manifester ?

- Manifester démontre à tous que cette question nous tient à cœur. En manifestant, nous prouvons que nous sommes prêts à consacrer du temps et de l'énergie à la diffusion de notre message.
- Il n’est pas rare que les manifestations aient une [influence positive](https://www.socialchangelab.org/_files/ugd/503ba4_052959e2ee8d4924934b7efe3916981e.pdf) sur l’opinion publique, le vote, l’attitude des entreprises et la loi.
- [La grande majorité des gens soutiennent](https://today.yougov.com/politics/articles/31718-do-protesters-want-help-or-hurt-america) les manifestations pacifiques et non violentes.
- Il n'y a [aucun "retour de bâton"](https://journals.sagepub.com/doi/full/10.1177/2378023120925949) sauf si la manifestation [dégénère en violences](https://news.stanford.edu/stories/2018/10/how-violent-protest-can-backfire). Nos manifestations sont pacifiques et non violentes.
- C'est une expérience de lien social. Vous rencontrez d'autres personnes qui partagent vos préoccupations et votre volonté d'agir.
- Lisez [cet excellent article](https://forum.effectivealtruism.org/posts/4ez3nvEmozwPwARr9/a-case-for-the-effectiveness-of-protest) pour en savoir plus sur l'efficacité des manifestations.

Si vous voulez [organiser une manifestation](https://pauseai.info/organizing-a-protest), nous pouvons vous apporter conseils et ressources.

### Quelle est la probabilité que l'apparition d'une superintelligence ait de graves conséquences, y compris un risque d'extinction ?

Nous avons compilé [une liste de valeurs "p(doom)"](https://pauseai.info/pdoom) (probabilité de scénarios catastrophiques) provenant de divers experts renommés dans le domaine.

Les chercheurs en sécurité de l'IA (qui sont experts du sujet) sont partagés, [leurs estimations allant de 2% à 97% avec une moyenne de 30%](https://web.archive.org/web/20221013014859/https://www.alignmentforum.org/posts/QvwSr5LsxyDeaPK5s/existential-risk-from-ai-survey-results). Notez qu'aucun des chercheurs en sécurité interrogés ne croit en une probabilité de 0%. Un biais de sélection est cependant possible: ceux qui travaillent dans le domaine de la sécurité de l'IA le font probablement car ils redoutent les conséquences néfastes de l'IA.

Si l'on interroge les chercheurs en IA en général (qui ne sont pas spécialistes en sécurité), ce chiffre tombe à [une moyenne d'environ 14%](https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/), avec une médiane de 5%. Une minorité, environ 20%, pense que le problème d'alignement n'est ni réel ni important. Là encore, un biais de sélection inverse est possible : ceux qui travaillent en IA le font sans doute car ils considèrent que les conséquences de l'IA sur le monde seront uniquement bénéfiques.

_Imaginez qu'on vous propose d'essayer un nouvel avion._ Les ingénieurs estiment les risques de crash à 14%. Monteriez-vous à bord ? C'est plus ou moins la situation actuelle, nous embarquons tous à bord du même avion.

### Combien de temps nous reste-t-il avant l'émergence d'une superintelligence ?

Cela pourrait prendre des mois, ou bien des décennies, personne n'en est certain. Ce que nous savons, c'est que les progrès dans le domaine de l'IA sont souvent largement sous-estimés. Il y a seulement trois ans, nous pensions qu'il faudrait attendre 2055 pour voir des modèles capables de réussir un test SAT (équivalent PISA aux Etats-Unis). Nous y sommes parvenus dès avril 2023. Il semble souhaitable d’ agir comme si le temps nous était compté afin de ne pas être pris au dépourvu.

[En savoir plus sur l'urgence de la situation.](https://pauseai.info/urgency)

### Si nous appliquons un moratoire, qu'en est-il de la Chine ?

La Chine a actuellement les réglementations les plus strictes au monde en matière d'IA. Les [chatbots sont interdits](https://www.reuters.com/technology/chinas-slow-ai-roll-out-points-its-tech-sectors-new-regulatory-reality-2023-07-12/) et [l'entraînement sur des données internet n'était pas autorisé](https://cointelegraph.com/news/china-sets-stricter-rules-training-generative-ai-models) jusqu'en [septembre 2023](https://asia.nikkei.com/Business/Technology/China-approves-AI-chatbot-releases-but-will-it-unleash-innovation). Le gouvernement chinois, avec son mode de régime autoritaire, a bien plus de raisons de craindre les impacts incontrôlables et imprévisibles de l'IA que nous. Lors de la réunion du Conseil de sécurité des Nations unies sur la sécurité de l'IA, la Chine a été le seul pays à mentionner la possibilité d’instaurer un moratoire..

De plus, nous appelons à un moratoire international , imposé par un traité. Un tel traité doit également être signé par la Chine. Si le traité garantit que d'autres nations s'arrêteront aussi, et qu'il y a des mécanismes de contrôle et des mesures de mise en vigueur suffisantes, la Chine y sera probablement favorable.

### OpenAI et Google semblent appeler de leurs vœux une réglementation. Pourquoi s’opposer à eux ?

Nous saluons les appels [d'OpenAI](https://openai.com/index/governance-of-superintelligence/) et de [Google](https://www.ft.com/content/8be1a975-e5e0-417d-af51-78af17ef4b79) pour demander une réglementation internationale vis-à-vis de l'IA. Cependant, nous pensons que les propositions actuelles ne suffiront pas à éviter une catastrophe. Google et Microsoft n'ont pas encore reconnu publiquement les risques existentiels liés à l'IA. Seul OpenAI [mentionne explicitement le risque d'extinction](https://openai.com/index/governance-of-superintelligence/). Cependant, leur stratégie est très claire: un moratoire est impossible, nous devons d'abord créer une superintelligence avant de penser à de sérieuses régulations. Mais il avouent eux-même [ne pas avoir résolu le problème d'alignemen](https://www.youtube.com/watch?t=1478&v=L_Guz73e6fw&feature=youtu.be)t et les derniers développements prouvent [qu'ils ne traitent pas ce problème avec le sérieux qu'il mérite](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html). Ces entreprises sont engagées dans une course contre la montre au détriment de la sécurité, sacrifiée pour un avantage concurrentiel. C'est le résultat de la dynamique du marché. Nous devons encourager les gouvernements à intervenir et à mettre en place des politiques internationales pour [éviter les pires scénarios](/propositions).

### Les entreprises d'IA prétendent-elles que le risque existentiel est réel pour nous manipuler ?

Nous ne pouvons pas être certains des motivations de ces entreprises, et nous savons qu'elles **ne sont pas à l'origine de la mise en avant des risques existentiels liés à l'IA**. Les signaux d'alerte venaient des scientifiques, militants et ONG. Jetons un œil à la chronologie.

Depuis le début des années 2000, diverses personnes ont alerté sur ce risque existentiel, comme Eliezer Yudkowsky, Nick Bostrom, Stuart Russell, Max Tegmark, et bien d'autres. Ils n'avaient aucun produit à vendre, ils étaient simplement préoccupés par l'avenir de l'humanité.

Les entreprises d'IA n'ont commencé à mentionner les risques existentiels que très récemment.

Sam Altman est une exception. Sur son blog personnel, il a exploré [l'idée du risque existentiel dès 2015](https://blog.samaltman.com/machine-intelligence-part-1), avant même de fonder OpenAI. Pendant les années qui ont suivi, il n'a quasiment plus jamais évoqué explicitement ce risque. Lors de son audition devant le Sénat Américain le 16 mai 2023, interrogé sur cet article de blog, il a soigneusement évité la question en préférant parler des emplois et de l'économie.

En mai 2023, tout a changé :

- Le 1er mai, le pionnier de l'IA Geoffrey Hinton [démissionne de Google](https://fortune.com/2023/05/01/godfather-ai-geoffrey-hinton-quit-google-regrets-lifes-work-bad-actors/) pour alerter le public sur la possibilité de risques existentiels.
- Le 20 mai, [la première manifestation de Pause IA](https://pauseai.info/openai-protest) a lieu devant le siège d'OpenAI.
- Le 22 mai, OpenAI publie [un article de blog sur la gouvernance de la superintelligence](https://openai.com/index/governance-of-superintelligence/), et mentionne le risque existentiel pour la première fois.
- Le 24 mai, l'ancien PDG de Google Eric Schmidt reconnaît la possibilité de risques existentiels.
- Le 30 mai, le centre pour la sécurité de l'IA publie [une déclaration sur les risques existentiels](https://www.safe.ai/work/statement-on-ai-risk), incluant des employés d'OpenAI, Google et Microsoft.

Ces entreprises ont été lentes à reconnaître la possibilité de risques existentiels, alors que beaucoup de leurs employés en étaient conscients depuis des années. Selon nous, les entreprises d'IA ont simplement réagi à l'émergence des risques existentiels dans le discours public et ont n'ont apporté leur réponse que lorsque le sujet est devenu inévitable.

Mais les incitations commerciales vont à contre-courant : il n'est pas dans l'intérêt de ces entreprises que le public s'inquiète des dangers de leurs produits. Presque toutes minimisent les risques pour attirer clients et investisseurs. Combien de régulations et de négativité risquent-elles d'attirer en admettant ces dangers ? Et une entreprise comme OpenAI consacrerait-elle [20% de sa puissance de calcul informatique](https://openai.com/index/introducing-superalignment/) à la sécurité de l'IA si elle ne croyait pas en ces dangers ?

Notre interprétation est que les entreprises d'IA ont signé cette déclaration parce qu'_elles savent que les risques existentiels sont un problème à prendre très au sérieux_.

Une raison majeure pour laquelle de nombreuses personnes ne veulent toujours pas croire que les risques existentiels sont une préoccupation réelle ? Parce que la reconnaissance d'un tel danger est une énorme charge mentale.

[En savoir plus sur la charge mentale des risques existentiels.](https://pauseai.info/psychology-of-x-risk)

### Je veux aider ! Que puis-je faire ?

Il y a de nombreuses choses que [vous pouvez faire](/agir). À titre individuel, [rédigez une lettre](https://pauseai.info/writing-a-letter), [distribuez des tracts](https://pauseai.info/flyering), sensibilisez votre entourage, prenez part à [une manifestation](https://pauseai.info/protests) ou [faites un don](/dons) ! Mais plus important : vous pouvez [rejoindre Pause IA](/nous-rejoindre) et coordonner vos actions avec d'autres personnes membres. Si vous souhaitez vous impliquer davantage, vous pouvez devenir bénévole et [intégrer une de nos équipes.](https://pauseai.info/teams)

Même confrontés à la perspective de risques existentiels, il reste de l'espoir et du travail à accomplir. 💪
